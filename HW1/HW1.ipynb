{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Word Frequencies\n",
    "\n",
    "## Challenge\n",
    "Can we identify different types of text documents based on the frequency of their words? Can we identify different authors, styles, or disciplines like medical versus information technology? The assignment is to compute word frequencies for different types of documents, and to develop patterns for document classification.\n",
    "\n",
    "## Tasks\n",
    "1. Write Python code to load different text documents and compute word frequencies. The most frequent words should be at the beginning of the list.\n",
    "2. Identify a small (about 5 to 10) words that could represent a particular type of document.\n",
    "3. Show how different types have different word lists (\"signatures\").\n",
    "4. Discuss results and the feasibilty of this method.\n",
    "\n",
    "## Deliverable\n",
    "Use this notebook to implement your assignment. Please, observe the following:\n",
    "1. Your notebook should have the completly executed code and results.\n",
    "2. Please, organize your notebook to tell the story. Remove unnecessary clutter, test code, and anything that does not belong to the story.\n",
    "3. Save your notebook in a directory named `HW1` in `MSA8010F16` in your *home* directory on the Hadoop Cluster. The path should be `~/MSA8010F16/HW1/HW1.ipynb`\n",
    "4. Also save the notebook in HTML as `~/MSA8010F16/HW1/HW1.html`\n",
    "5. All file names are *case sensitive*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### I want to first build a function that will return the list of words appearing in a document. This requires removing spaces, linebreaks, and punctuation. I must define the list of punctuation marks to remove, which I defined with the variable rmvpunc. I have chosen to leave apostrophes in the words to maintain contractions. In order to remove the empty strings that result, I found it possible to use a list comprehension to filter those out.\n",
    "\n",
    "##### Additionally, I later found that many text documents have '\\r' in addition or in lieu of '\\n' as a linebreak, so I used a second replace to eliminate those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def makelist(src):\n",
    "    rmvpunc = string.punctuation.replace(\"'\", \" \")\n",
    "    txt = src.lower()\n",
    "    for c in rmvpunc:\n",
    "        txt = txt.replace(c, '\\n')\n",
    "        txt = txt.replace('\\r','\\n')\n",
    "    wordlist = [i for i in txt.split('\\n') if i != '']\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test this function on a small text file I created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these',\n",
       " 'words',\n",
       " 'are',\n",
       " 'on',\n",
       " 'the',\n",
       " 'first',\n",
       " 'line',\n",
       " 'these',\n",
       " 'words',\n",
       " 'are',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'line',\n",
       " 'these',\n",
       " 'words',\n",
       " 'are',\n",
       " 'on',\n",
       " 'the',\n",
       " 'third',\n",
       " 'line',\n",
       " 'the',\n",
       " 'fourth',\n",
       " 'line',\n",
       " 'is',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'here',\n",
       " 'goes',\n",
       " 'the',\n",
       " 'fifth',\n",
       " 'line',\n",
       " \"what's\",\n",
       " 'the',\n",
       " 'point',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sixth',\n",
       " 'line',\n",
       " 'lucky',\n",
       " 'seventh',\n",
       " 'line',\n",
       " 'baby',\n",
       " 'eighth',\n",
       " 'line',\n",
       " 'material',\n",
       " 'appears',\n",
       " 'here']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makelist(open('text.txt').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I need a function that counts the words from the word list. I'll use a dictionary since that makes the most sense to me to count word frequency, but then I'll return a list of tuples which will make for easier sorting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('second', 1),\n",
       " (\"what's\", 1),\n",
       " ('line', 8),\n",
       " ('material', 1),\n",
       " ('these', 3),\n",
       " ('lucky', 1),\n",
       " ('appears', 1),\n",
       " ('goes', 1),\n",
       " ('fourth', 1),\n",
       " ('fifth', 1),\n",
       " ('on', 3),\n",
       " ('are', 3),\n",
       " ('here', 2),\n",
       " ('point', 1),\n",
       " ('seventh', 1),\n",
       " ('baby', 1),\n",
       " ('this', 1),\n",
       " ('is', 1),\n",
       " ('of', 1),\n",
       " ('words', 3),\n",
       " ('sixth', 1),\n",
       " ('eighth', 1),\n",
       " ('first', 1),\n",
       " ('third', 1),\n",
       " ('stuff', 1),\n",
       " ('the', 7)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wordcount(wordlist):\n",
    "    freq_dict = {}\n",
    "    for i in wordlist:\n",
    "        if i not in freq_dict:\n",
    "            freq_dict[i] = 1\n",
    "        else:\n",
    "            freq_dict[i] += 1\n",
    "    return list(freq_dict.items())\n",
    "\n",
    "wordcount(makelist(open('text.txt').read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I can write a function that sorts the list of tuples and ties everything together by calling my previous functions and then printing the twenty most frequent words along with how many times each word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line - 8\n",
      "the - 7\n",
      "these - 3\n",
      "on - 3\n",
      "are - 3\n",
      "words - 3\n",
      "here - 2\n",
      "second - 1\n",
      "what's - 1\n",
      "material - 1\n",
      "lucky - 1\n",
      "appears - 1\n",
      "goes - 1\n",
      "fourth - 1\n",
      "fifth - 1\n",
      "point - 1\n",
      "seventh - 1\n",
      "baby - 1\n",
      "this - 1\n",
      "is - 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def topten(src):\n",
    "    x = wordcount(makelist(src))\n",
    "    x.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i in range(20):\n",
    "        print (x[i][0],\"-\",x[i][1])\n",
    "    print ('\\n')\n",
    "\n",
    "topten(open('text.txt').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run the function with the locally stored Shakespeare text file and begin building lists to compare the \"signatures\" of various types of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the - 27618\n",
      "and - 26771\n",
      "i - 20296\n",
      "to - 19685\n",
      "of - 18183\n",
      "a - 14565\n",
      "you - 13645\n",
      "my - 12473\n",
      "that - 11135\n",
      "in - 11000\n",
      "is - 9596\n",
      "not - 8733\n",
      "for - 8251\n",
      "with - 7999\n",
      "me - 7771\n",
      "it - 7697\n",
      "be - 7089\n",
      "your - 6876\n",
      "his - 6853\n",
      "this - 6832\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topten(open('shakespeare.txt').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll also run the code with other fictional writings - the complete works of American author Mark Twain and English author Jane Austen.\n",
    "(These are pulled locally because of problems with 'urlopen' accessing documents on the gutenberg.org domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the - 155007\n",
      "and - 122526\n",
      "of - 79376\n",
      "a - 73415\n",
      "to - 71630\n",
      "it - 49657\n",
      "in - 48082\n",
      "i - 45342\n",
      "that - 39858\n",
      "was - 39596\n",
      "he - 32872\n",
      "is - 24821\n",
      "for - 22502\n",
      "his - 21730\n",
      "you - 21157\n",
      "with - 21046\n",
      "but - 20818\n",
      "not - 17218\n",
      "had - 16950\n",
      "as - 16525\n",
      "\n",
      "\n",
      "the - 28448\n",
      "to - 26119\n",
      "and - 24176\n",
      "of - 23051\n",
      "a - 14366\n",
      "her - 14031\n",
      "i - 13785\n",
      "in - 12226\n",
      "was - 11783\n",
      "it - 10834\n",
      "she - 10710\n",
      "not - 9112\n",
      "that - 8928\n",
      "be - 8706\n",
      "you - 8413\n",
      "he - 7758\n",
      "had - 7669\n",
      "as - 7608\n",
      "for - 7221\n",
      "with - 6382\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topten(open('twain.txt').read())\n",
    "topten(open('austen.txt').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems these fictional works mostly use \"structural\" English language words, without any particularly unique words appearing in the lists.\n",
    "\n",
    "#### Let's look at some other types of documents to compare - \n",
    "* a philosophical document from Immanuel Kant\n",
    "* a medical textbook\n",
    "* a scientific document, Darwin's 'Origin of Species,'\n",
    "* a legal document 'Prize Cases Decided in the US Supreme Court, 1789-1918.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immanuel Kant\n",
      "the - 15918\n",
      "of - 13074\n",
      "to - 6208\n",
      "in - 5942\n",
      "a - 5077\n",
      "is - 4876\n",
      "and - 4838\n",
      "which - 3424\n",
      "that - 3095\n",
      "it - 2950\n",
      "as - 2879\n",
      "be - 2378\n",
      "this - 2232\n",
      "not - 1934\n",
      "we - 1924\n",
      "but - 1784\n",
      "for - 1690\n",
      "all - 1430\n",
      "an - 1405\n",
      "by - 1370\n",
      "\n",
      "\n",
      "Textbook of Medical Physiology\n",
      "the - 65627\n",
      "of - 39137\n",
      "in - 20205\n",
      "and - 18995\n",
      "to - 14877\n",
      "is - 12555\n",
      "a - 10528\n",
      "that - 6956\n",
      "as - 5889\n",
      "by - 5845\n",
      "are - 5441\n",
      "this - 5092\n",
      "for - 4842\n",
      "from - 4748\n",
      "blood - 4607\n",
      "with - 3226\n",
      "or - 3208\n",
      "1 - 2961\n",
      "pressure - 2961\n",
      "cells - 2897\n",
      "\n",
      "\n",
      "Charles Darwin\n",
      "the - 14562\n",
      "of - 10427\n",
      "and - 5852\n",
      "in - 5414\n",
      "to - 4752\n",
      "a - 3379\n",
      "that - 2749\n",
      "as - 2230\n",
      "have - 2114\n",
      "be - 2099\n",
      "is - 2069\n",
      "on - 1952\n",
      "species - 1922\n",
      "by - 1824\n",
      "which - 1787\n",
      "or - 1655\n",
      "are - 1646\n",
      "it - 1526\n",
      "for - 1465\n",
      "with - 1464\n",
      "\n",
      "\n",
      "Court Decisions\n",
      "the - 33939\n",
      "of - 18644\n",
      "to - 11284\n",
      "and - 9345\n",
      "in - 7939\n",
      "a - 7002\n",
      "that - 5609\n",
      "is - 4367\n",
      "it - 4017\n",
      "be - 3872\n",
      "was - 3604\n",
      "by - 3513\n",
      "as - 3061\n",
      "not - 2968\n",
      "court - 2672\n",
      "on - 2638\n",
      "this - 2495\n",
      "which - 2350\n",
      "for - 2330\n",
      "' - 2165\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "print (\"Immanuel Kant\")\n",
    "topten(urlopen('http://www.textfiles.com/etext/AUTHORS/KANT/kant-critique-142.txt').read().decode())\n",
    "print (\"Textbook of Medical Physiology\")\n",
    "topten(urlopen('https://archive.org/stream/GuytonAndHallTextbookOfMedicalPhysiologyRentalEBookNodrm/Guyton%20and%20Hall%20Textbook%20of%20Medical%20Physiology%20Rental%20E-Book_nodrm_djvu.txt').read().decode())\n",
    "print (\"Charles Darwin\")\n",
    "topten(urlopen('http://www.loyalbooks.com/download/text/The-Origin-of-Species-by-Charles-Darwin.txt').read().decode())\n",
    "print (\"Court Decisions\")\n",
    "topten(urlopen('https://archive.org/stream/prizecasesdecide00scotuoft/prizecasesdecide00scotuoft_djvu.txt').read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The words 'the, of, and, in, to' appear in the ten most frequent words in every document I've tested, and generally comprise the top five. It would seem that any English document of sufficient length will ultimately contain these and other basic words most frequently, and thus these common words wouldn't contribute to a \"signature\" for a document. Writing in English simply requires these building blocks quite often to construct sentences.\n",
    "\n",
    "We begin to see more unique identifiers in the 11th-20th words once we get into more topical documents. For example, the medical text contains 'blood' and 'cells,' Darwin's work uses 'species' often, and court decisions contain the word 'court' often."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
